{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('.venv')",
   "metadata": {
    "interpreter": {
     "hash": "30f42aa134329466a731abbd038d485910fae0b3d001dee8c42beebbf09a5e0a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nprand\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input shape:  (42000, 784)\noutput shape: (42000, 1)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/train.csv\")\n",
    "dfn = df.to_numpy()\n",
    "dfn = np.hsplit(dfn, [1])\n",
    "y = dfn[0]\n",
    "x = dfn[1]\n",
    "print(f\"input shape:  {x.shape}\")\n",
    "print(f\"output shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "output shape: (42000, 10)\n"
     ]
    }
   ],
   "source": [
    "# normalize our inputs\n",
    "row_sums = x.sum(axis=1)\n",
    "# could use np.atleast_2d(row_sums) to ensure that row_sums is (3,1) and not (3,)\n",
    "x = x / row_sums[:, np.newaxis]\n",
    "\n",
    "# change our outputs to be vectorized\n",
    "def expected_transform(expected):\n",
    "    new_arr = np.zeros((len(expected), 10))\n",
    "    for i in range(len(expected)):\n",
    "        new_arr[i][expected[i][0]] = 1\n",
    "\n",
    "    return new_arr\n",
    "\n",
    "y = expected_transform(y)\n",
    "print(f\"output shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2.90667863e-04 1.06930731e-04 1.58699276e-02 1.17263785e-01\n 8.66468688e-01]\n1.0\n"
     ]
    }
   ],
   "source": [
    "# an implementation of sigmoid function\n",
    "#\n",
    "# Thanks to source:\n",
    "# https://stackoverflow.com/questions/3985619/how-to-calculate-a-logistic-sigmoid-function-in-python\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# assumes x is already sigmoid(x)\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * sigmoid(1 - x)\n",
    "\n",
    "# an implementation of a ReLU (rectified linear unit) \n",
    "# function\n",
    "#\n",
    "# Thanks to source: \n",
    "# https://stackoverflow.com/questions/32109319/how-to-implement-the-relu-function-in-numpy\n",
    "#\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "# an implementation of the ReLU derivative\n",
    "#\n",
    "# Thanks to source:\n",
    "# https://stackoverflow.com/questions/46411180/implement-relu-derivative-in-python-numpy\n",
    "def relu_derivative(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "# an implementation of the softmax function\n",
    "#\n",
    "# Thanks to source:\n",
    "# https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    pass\n",
    "\n",
    "softmax_test = np.array([1, 0, 5, 7, 9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Layer 1 weights shape (128, 784)\nLayer 2 weights shape (10, 128)\nLayer 1 bias shape    (128, 1)\nLayer 2 bias shape    (10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Generates a distribution of random numbers in some range,\n",
    "# specified by the init_type parameter.\n",
    "#   n is the number of input nodes \n",
    "#   m is the number of output nodes.\n",
    "#   U is the uniform distribution. \n",
    "#   G is the gaussian or normal distribution.\n",
    "#\n",
    "# Thanks to source: \n",
    "# https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "# \n",
    "# Initialization type (init_type) can be:\n",
    "#\n",
    "# Good for Sigmoid and Tanh activation functions.\n",
    "# \"xavier\"\n",
    "# Xavier Glorot uses the formula: \n",
    "#       weight = U[-(1/sqrt(n)), 1/sqrt(n)]\n",
    "#\n",
    "# \"nxavier\"\n",
    "# Normalized Xavier Glorot uses the formula:\n",
    "#       weight = U[-(sqrt(6)/sqrt(n+m)), sqrt(6)/sqrt(n+m)]\n",
    "# \n",
    "# Good for ReLU activation funtions.\n",
    "# \"he\"\n",
    "# Kaiming He uses the formula:\n",
    "#       weight = G(0.0, sqrt(2/n))\n",
    "#\n",
    "#\n",
    "# \n",
    "def initialize_layer_weights(n, m, init_type=\"xavier\"):\n",
    "    if \"xavier\" in init_type:\n",
    "        numbers = nprand.rand(m, n)\n",
    "        if init_type == \"xavier\":\n",
    "            lower, upper = -(1.0 / math.sqrt(n)), (1.0 / math.sqrt(n))\n",
    "        else:\n",
    "            lower, upper = -(math.sqrt(6.0) / math.sqrt(n + m)), (math.sqrt(6.0) / math.sqrt(n + m))\n",
    "        scaled = lower + numbers * (upper - lower)\n",
    "    else:\n",
    "        std = math.sqrt(2.0 / n)\n",
    "        numbers = nprand.randn(m, n)\n",
    "        scaled = numbers * std\n",
    "\n",
    "    return scaled\n",
    "\n",
    "layers = [x.shape[1], 128, 10]\n",
    "\n",
    "# initialize our layer 1 and layer 2 weights\n",
    "# layer 1 uses a ReLU function, so use \"he\" init type\n",
    "# layer 2 uses a sigmoid function, so use the xavier or nxavier type\n",
    "l1_w = initialize_layer_weights(layers[0], layers[1], \"he\")\n",
    "l2_w = initialize_layer_weights(layers[1], layers[2], \"nxavier\")\n",
    "\n",
    "# initialize our bias to 0 for all layers\n",
    "l1_b = np.zeros((layers[1], 1))\n",
    "l2_b = np.zeros((layers[2], 1))\n",
    "\n",
    "print(f\"Layer 1 weights shape {l1_w.shape}\")\n",
    "print(f\"Layer 2 weights shape {l2_w.shape}\")\n",
    "print(f\"Layer 1 bias shape    {l1_b.shape}\")\n",
    "print(f\"Layer 2 bias shape    {l2_b.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10, 20)\n"
     ]
    }
   ],
   "source": [
    "# forward propegate\n",
    "mini_batch_x = x[0:20].T\n",
    "mini_batch_y = y[0:20].T\n",
    "z1 = l1_w.dot(mini_batch_x) + l1_b\n",
    "a1 = relu(z1)\n",
    "z2 = l2_w.dot(a1) + l2_b\n",
    "a2 = sigmoid(z2)\n",
    "print(a2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10, 128)\n(10, 1)\n"
     ]
    }
   ],
   "source": [
    "da2 = (a2 - mini_batch_y)\n",
    "dz2 = da2 * sigmoid_derivative(z2)\n",
    "nabla_l2_b = np.sum(dz2, axis=1, keepdims=True)\n",
    "nabla_l2_w = np.dot(dz2, a1.T)\n",
    "\n",
    "print(nabla_l2_w.shape)\n",
    "print(nabla_l2_b.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(128, 784)\n(128, 1)\n"
     ]
    }
   ],
   "source": [
    "da1 = np.dot(l2_w.T, dz2)\n",
    "dz1 = da1 * relu_derivative(a1)\n",
    "nabla_l1_b = np.sum(dz1, axis=1, keepdims=True)\n",
    "nabla_l1_w = np.dot(dz1, mini_batch_x.T)\n",
    "print(nabla_l1_w.shape)\n",
    "print(nabla_l1_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.2\n",
    "\n",
    "l1_w = l1_w - learning_rate * nabla_l1_w\n",
    "l2_w = l2_w - learning_rate * nabla_l2_w\n",
    "l1_b = l1_b - learning_rate * nabla_l1_b\n",
    "l2_b = l2_b - learning_rate * nabla_l2_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20.28716753860027\n",
      "20.28716753851238\n",
      "20.28716753842451\n",
      "20.287167538336657\n",
      "20.287167538248823\n",
      "20.287167538161004\n",
      "20.287167538073202\n",
      "20.287167537985415\n",
      "20.287167537897645\n",
      "20.287167537809907\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-00d48925d788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmini_batch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml1_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml1_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mz2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml2_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# put it all together:\n",
    "\n",
    "mini_batch_size = 47000\n",
    "for i in range(10000):\n",
    "    for i in range(0, len(x), mini_batch_size):\n",
    "        mini_batch_x = x[i: i + mini_batch_size].T\n",
    "        mini_batch_y = y[i: i + mini_batch_size].T\n",
    "\n",
    "        z1 = l1_w.dot(mini_batch_x) + l1_b\n",
    "        a1 = relu(z1)\n",
    "        z2 = l2_w.dot(a1) + l2_b\n",
    "        a2 = sigmoid(z2)\n",
    "\n",
    "        cost = np.mean(np.square(a2 - mini_batch_y))\n",
    "        print(cost)\n",
    "\n",
    "        da2 = (a2 - mini_batch_y)\n",
    "        dz2 = da2 * sigmoid_derivative(z2)\n",
    "        nabla_l2_b = (1/mini_batch_size) * np.sum(dz2, axis=1, keepdims=True)\n",
    "        nabla_l2_w = (1/mini_batch_size) * np.dot(dz2, a1.T)\n",
    "\n",
    "        da1 = np.dot(l2_w.T, dz2)\n",
    "        dz1 = da1 * relu_derivative(a1)\n",
    "        nabla_l1_b = (1/mini_batch_size) * np.sum(dz1, axis=1, keepdims=True)\n",
    "        nabla_l1_w = (1/mini_batch_size) * np.dot(dz1, mini_batch_x.T)\n",
    "\n",
    "        learning_rate = 1\n",
    "\n",
    "        l1_w = l1_w - learning_rate * nabla_l1_w\n",
    "        l2_w = l2_w - learning_rate * nabla_l2_w\n",
    "        l1_b = l1_b - learning_rate * nabla_l1_b\n",
    "        l2_b = l2_b - learning_rate * nabla_l2_b\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.00000000e+00 5.71181227e-01 8.92892661e-02 2.20493773e-01\n",
      " 1.13409495e+00 0.00000000e+00 9.75738221e-01 4.82724055e-01\n",
      " 0.00000000e+00 1.39296050e+00 0.00000000e+00 5.50516696e-01\n",
      " 2.09571749e-01 0.00000000e+00 0.00000000e+00 2.90848570e-01\n",
      " 2.38597204e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 9.67062418e-01 7.78219935e-01 1.95200083e-01 1.87265931e-01\n",
      " 5.81074144e-04 4.21591584e-01 0.00000000e+00 0.00000000e+00\n",
      " 4.26906751e-01 5.27145175e-02 0.00000000e+00 1.71380700e+00\n",
      " 0.00000000e+00 1.69375077e-01 0.00000000e+00 9.58988287e-02\n",
      " 1.99226501e+00 1.54224467e+00 6.07234664e-01 1.86027197e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 2.21638904e+00 4.42723528e-02\n",
      " 6.33513289e-01 0.00000000e+00 1.57791338e-01 1.91743408e-01\n",
      " 1.06712493e+00 5.44608659e-02 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 2.17533907e-01 0.00000000e+00\n",
      " 0.00000000e+00 4.86856158e-01 1.49373625e+00 7.82654347e-02\n",
      " 0.00000000e+00 0.00000000e+00 8.20555973e-01 9.23679176e-01\n",
      " 3.37978857e-01 3.39040888e-01 0.00000000e+00 2.04141066e-01\n",
      " 4.06219679e-01 8.87122986e-01 5.14043815e-04 1.34554100e+00\n",
      " 0.00000000e+00 0.00000000e+00 5.54678340e-01 8.16531298e-01\n",
      " 2.50809868e+00 8.33436592e-01 0.00000000e+00 0.00000000e+00\n",
      " 4.61699973e-01 9.91089955e-01 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 1.82537664e+00 5.33735121e-01 5.76734863e-01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 7.60736164e-01 1.48830148e+00 0.00000000e+00 1.96674643e+00\n",
      " 0.00000000e+00 0.00000000e+00 4.66353984e-01 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 2.55477647e+00 9.06867258e-03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 8.12570870e-01 9.80923972e-01 0.00000000e+00 3.10103390e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 2.35789630e-01 0.00000000e+00 8.22693715e-02]\n",
      "[0.99999951 0.99999951 0.99999951 0.99999951 0.99999951 0.99999951\n",
      " 0.9999995  0.99999952 0.99999951 0.99999951]\n",
      "5174\n"
     ]
    }
   ],
   "source": [
    "# check out accuracy\n",
    "y = dfn[0]\n",
    "\n",
    "z1 = l1_w.dot(x.T) + l1_b\n",
    "a1 = relu(z1)\n",
    "z2 = l2_w.dot(a1) + l2_b\n",
    "a2 = sigmoid(z2)\n",
    "\n",
    "count_correct = 0\n",
    "\n",
    "print(a1.T[0])\n",
    "print(a2.T[1])\n",
    "\n",
    "for i in range(len(x)):\n",
    "    predicted = np.where(a2.T[i]==np.amax(a2.T[i]))[0][0]\n",
    "    expected = y[i][0]\n",
    "    if predicted == expected:\n",
    "        count_correct += 1\n",
    "\n",
    "print(count_correct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}